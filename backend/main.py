# -*- coding: utf-8 -*-
# LOAD ENVIRONMENT VARIABLES FIRST
from pathlib import Path
from dotenv import load_dotenv

# Find .env file (project root)
env_path = Path(__file__).parent.parent / ".env"
load_dotenv(dotenv_path=env_path)
print(f"[STARTUP] Loaded .env from: {env_path} (exists: {env_path.exists()})")

# IMPORT UTF-8 CONFIG FIRST - before any other imports
import backend.utf8_config  # noqa: F401

import os
import sys
import io

import uuid
from datetime import datetime, timezone
from fastapi import HTTPException


def get_debug_log_path() -> str | None:
    """Get debug log path from environment or use relative path.
    Returns None if file logging is disabled (for Docker/production)."""
    if os.getenv("DISABLE_FILE_LOGGING", "").lower() in ("true", "1", "yes"):
        return None
    return os.getenv("DEBUG_LOG_PATH", str(Path(__file__).parent.parent / "debug.log"))


def _write_debug_log(message: str) -> None:
    """Write to debug log file if file logging is enabled."""
    debug_log_path = get_debug_log_path()
    if debug_log_path is None:
        return
    try:
        with open(debug_log_path, "a", encoding="utf-8") as f:
            f.write(f"{message} - {datetime.now()}\n")
            f.flush()
    except Exception:
        pass  # Silently ignore file logging errors in production

"""
F&B Operations Agent - FastAPI Backend
MVP Phase 1 - Entry point
"""

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI(
    title="F&B Operations Agent API",
    description="AI-powered staffing forecasting for restaurants",
    version="0.1.0"
)

# Add middleware to log all requests
@app.middleware("http")
async def log_requests(request, call_next):
    import sys
    import logging
    
    logger = logging.getLogger("uvicorn")
    logger.info(f"[MIDDLEWARE] Request: {request.method} {request.url.path}")
    _write_debug_log(f"[MIDDLEWARE] Request: {request.method} {request.url.path}")
    
    response = await call_next(request)
    
    logger.info(f"[MIDDLEWARE] Response: {response.status_code}")
    _write_debug_log(f"[MIDDLEWARE] Response: {response.status_code}")
    
    return response

# CORS for frontend
# Allow all origins in development, restrict in production
cors_origins = os.getenv(
    "CORS_ORIGINS",
    "http://localhost:3000,http://localhost:8000,http://127.0.0.1:3000,http://127.0.0.1:8000"
).split(",")

app.add_middleware(
    CORSMiddleware,
    allow_origins=cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
)

# F&B Agent API routes (restaurant profile, predictions, feedback)
from backend.api.routes import router as api_router
from backend.api.restaurant_profile_routes import router as restaurant_profile_router

app.include_router(api_router)
app.include_router(restaurant_profile_router)

@app.get("/")
async def root():
    return {
        "name": "F&B Operations Agent API",
        "version": "1.0.0",
        "status": "running",
        "docs": "/docs"
    }

@app.get("/health")
async def health():
    return {"status": "healthy", "version": "1.0.0"}

@app.get("/test/claude")
async def test_claude():
    """Test Claude API connection"""
    from backend.utils.claude_client import ClaudeClient
    async with ClaudeClient() as client:
        return await client.test_connection()

@app.get("/test/qdrant")
async def test_qdrant():
    """Test Qdrant connection"""
    from .utils.qdrant_client import QdrantManager
    client = QdrantManager()
    return await client.test_connection()

# ============================================
# PHASE 1: Prediction Endpoints
# ============================================

import math

from backend.models.schemas import (
    PredictionRequest,
    PredictionResponse,
    BatchPredictionRequest,
    Reasoning,
    StaffRecommendation,
    StaffDelta,
    AccuracyMetrics,
    ServiceType,
)
from backend.agents.demand_predictor import get_demand_predictor
from backend.api.prediction_store import store_prediction_for_feedback

@app.post("/predict", response_model=PredictionResponse)
async def create_prediction(request: PredictionRequest):
    """
    Create a staffing prediction
    
    Phase 1: Basic prediction (mocked data)
    Phase 2: Full integration (real patterns, APIs)
    """
    import logging
    logger = logging.getLogger("uvicorn")
    
    try:
        logger.info(f"[PREDICT] Request: {request.service_date} ({request.service_type})")
        _write_debug_log(f"[PREDICT] Request: {request.service_date} ({request.service_type})")
        
        # Get demand predictor
        predictor = get_demand_predictor()
        
        # Generate prediction
        result = await predictor.predict(request)
        logger.info(f"[PREDICT] Result: {result.get('predicted_covers')} covers")
        
        # TODO Hour 3-4: Add reasoning engine + staff recommender
        # For now, return basic structure
        
        # Extract reasoning from predictor result (now includes Claude-generated reasoning)
        reasoning_data = result.get("reasoning", {})
        patterns_from_result = reasoning_data.get("patterns_used", [])
        
        reasoning = Reasoning(
            summary=reasoning_data.get("summary", f"{int(result['confidence']*100)}% confidence"),
            patterns_used=patterns_from_result[:3] if patterns_from_result else [],
            confidence_factors=reasoning_data.get("confidence_factors", ["Historical patterns"])
        )
        
        # Create StaffRecommendation object from dynamic calculation
        staff_data = result.get("staff_recommendation", {})
        staff_recommendation = StaffRecommendation(
            servers=StaffDelta(
                recommended=staff_data.get("servers", {}).get("recommended", 7),
                usual=staff_data.get("servers", {}).get("usual", 7),
                delta=staff_data.get("servers", {}).get("delta", 0)
            ),
            hosts=StaffDelta(
                recommended=staff_data.get("hosts", {}).get("recommended", 2),
                usual=staff_data.get("hosts", {}).get("usual", 2),
                delta=staff_data.get("hosts", {}).get("delta", 0)
            ),
            kitchen=StaffDelta(
                recommended=staff_data.get("kitchen", {}).get("recommended", 3),
                usual=staff_data.get("kitchen", {}).get("usual", 3),
                delta=staff_data.get("kitchen", {}).get("delta", 0)
            ),
            rationale=staff_data.get("rationale", ""),
            covers_per_staff=staff_data.get("covers_per_staff", 0.0)
        )
        restaurant_context = None

        # Enrich with restaurant profile when available (IVA-52)
        try:
            from backend.api.routes import get_supabase
            supabase = get_supabase()
            profile_resp = (
                supabase.table("restaurant_profiles")
                .select("*")
                .eq("outlet_name", request.restaurant_id)
                .limit(1)
                .execute()
            )
            if profile_resp.data and len(profile_resp.data) > 0:
                profile = profile_resp.data[0]
                predicted = result["predicted_covers"]
                turns_dinner = profile.get("turns_dinner", 2.0)
                total_seats = profile["total_seats"]
                capacity_pct = round(
                    (predicted / (total_seats * turns_dinner)) * 100, 1
                )
                restaurant_context = {
                    "total_seats": total_seats,
                    "breakeven_covers": profile.get("breakeven_covers"),
                    "target_covers": profile.get("target_covers"),
                    "capacity_pct": capacity_pct,
                }
                # Override staff recommendation with profile-based calculation
                servers_rec = max(
                    math.ceil(predicted / profile["covers_per_server"]),
                    profile.get("min_foh_staff", 2),
                )
                hosts_rec = max(
                    math.ceil(predicted / profile["covers_per_host"]),
                    1,
                )
                kitchen_rec = max(
                    math.ceil(predicted / profile["covers_per_kitchen"]),
                    profile.get("min_boh_staff", 2),
                )
                staff_recommendation = StaffRecommendation(
                    servers=StaffDelta(
                        recommended=servers_rec,
                        usual=servers_rec,
                        delta=0,
                    ),
                    hosts=StaffDelta(
                        recommended=hosts_rec,
                        usual=hosts_rec,
                        delta=0,
                    ),
                    kitchen=StaffDelta(
                        recommended=kitchen_rec,
                        usual=kitchen_rec,
                        delta=0,
                    ),
                    rationale=staff_data.get("rationale", ""),
                    covers_per_staff=round(
                        predicted / (servers_rec + hosts_rec + kitchen_rec), 1
                    )
                    if (servers_rec + hosts_rec + kitchen_rec) > 0
                    else 0.0,
                )
        except Exception as e:
            logger.debug(f"[PREDICT] No restaurant profile for {request.restaurant_id}: {e}")

        # Extract accuracy_metrics from result
        accuracy_data = result.get("accuracy_metrics", {})
        accuracy_metrics = None
        if accuracy_data:
            # Convert tuple to list for prediction_interval if present
            if "prediction_interval" in accuracy_data and accuracy_data["prediction_interval"]:
                if isinstance(accuracy_data["prediction_interval"], tuple):
                    accuracy_data["prediction_interval"] = list(accuracy_data["prediction_interval"])
            accuracy_metrics = AccuracyMetrics(**accuracy_data)

        # Extraire range_low et range_high
        interval = accuracy_metrics.prediction_interval if accuracy_metrics else None
        range_low = interval[0] if interval and len(interval) >= 2 else result["predicted_covers"] - 10
        range_high = interval[1] if interval and len(interval) >= 2 else result["predicted_covers"] + 10

        # Extraire patterns pour stockage (convertis en dict si besoin)
        patterns_for_storage = []
        if reasoning_data and "patterns_used" in reasoning_data:
            raw = reasoning_data["patterns_used"]
            for p in raw:
                if isinstance(p, dict):
                    patterns_for_storage.append(p)
                elif hasattr(p, "model_dump"):
                    patterns_for_storage.append(p.model_dump())
                else:
                    patterns_for_storage.append(dict(p) if hasattr(p, "__iter__") else {})

        # Extraire confidence_factors
        confidence_factors = reasoning_data.get("confidence_factors", []) if reasoning_data else []

        # Stocker en Supabase pour feedback loop
        stored_id = None
        try:
            stored_id = store_prediction_for_feedback(
                restaurant_id=request.restaurant_id,
                service_date=request.service_date,
                service_type=request.service_type.value if hasattr(request.service_type, "value") else str(request.service_type),
                predicted_covers=result["predicted_covers"],
                confidence=result["confidence"],
                range_low=range_low,
                range_high=range_high,
                estimated_mape=accuracy_metrics.estimated_mape if accuracy_metrics else None,
                patterns=patterns_for_storage,
                confidence_factors=confidence_factors
            )
        except Exception as e:
            logger.warning(f"[PREDICT] Store for feedback failed: {e}")

        # Utiliser l'UUID Supabase si disponible, sinon fallback
        prediction_id = stored_id if stored_id else f"pred_{uuid.uuid4().hex[:8]}"

        # Return PredictionResponse
        return PredictionResponse(
            prediction_id=prediction_id,
            service_date=request.service_date,
            service_type=request.service_type,
            predicted_covers=result["predicted_covers"],
            confidence=result["confidence"],
            reasoning=reasoning,
            staff_recommendation=staff_recommendation,
            accuracy_metrics=accuracy_metrics,
            restaurant_context=restaurant_context,
            created_at=datetime.now(timezone.utc).isoformat()
        )
        
    except ValueError as e:
        error_detail = str(e).encode('utf-8', errors='replace').decode('utf-8', errors='replace')
        logger.warning(f"[PREDICT] ValueError: {error_detail}")
        raise HTTPException(status_code=422, detail=error_detail)
    except Exception as e:
        error_detail = str(e).encode('utf-8', errors='replace').decode('utf-8', errors='replace')
        logger.error(f"[PREDICT] Error: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)


@app.post("/predict/batch")
async def create_prediction_batch(request: BatchPredictionRequest):
    """
    Generate predictions for multiple dates at once.
    More efficient than calling /predict multiple times.
    Max 31 dates per request.
    """
    import logging
    logger = logging.getLogger("uvicorn")
    MAX_DATES = 31
    dates = request.dates[:MAX_DATES]
    try:
        st_enum = ServiceType(request.service_type.lower())
    except (ValueError, AttributeError):
        st_enum = ServiceType.DINNER

    predictions = []
    for d in dates:
        try:
            service_date = date.fromisoformat(d)
        except ValueError:
            continue
        single = PredictionRequest(
            restaurant_id=request.restaurant_id,
            service_date=service_date,
            service_type=st_enum,
        )
        try:
            resp = await create_prediction(single)
            out = resp.model_dump()
            out["date"] = d
            out["service_date"] = d
            predictions.append(out)
        except Exception as e:
            logger.warning(f"[PREDICT/BATCH] Skip date {d}: {e}")
            predictions.append({
                "date": d,
                "service_date": d,
                "predicted_covers": 0,
                "confidence": 0.0,
                "accuracy_metrics": {"prediction_interval": [0, 0]},
                "staff_recommendation": {},
            })
    return {
        "predictions": predictions,
        "count": len(predictions),
        "service_type": request.service_type,
        "restaurant_id": request.restaurant_id,
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)